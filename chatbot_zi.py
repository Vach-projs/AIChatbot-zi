# -*- coding: utf-8 -*-
"""chatbot-zi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15oI98BqpxWKJG2EUGdZLs4_cg_ccYxmZ
"""

#installs
!pip install wget --quiet

#imports
import os
import re
import wget
import random
import torch
import zipfile
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from tqdm.notebook import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on {device}")

#dataset
dataset_url = "http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
if not os.path.exists("cornell_movie_dialogs_corpus.zip"):
    wget.download(dataset_url)
    with zipfile.ZipFile("cornell_movie_dialogs_corpus.zip", 'r') as zip_ref:
        zip_ref.extractall(".")

#lines
def load_lines():
    lines = {}
    with open("cornell movie-dialogs corpus/movie_lines.txt", encoding='iso-8859-1') as f:
        for line in f:
            parts = line.strip().split(" +++$+++ ")
            if len(parts) == 5:
                lines[parts[0]] = parts[4]
    return lines

#converstaion
def load_conversations(lines):
    pairs = []
    with open("cornell movie-dialogs corpus/movie_conversations.txt", encoding='iso-8859-1') as f:
        for line in f:
            parts = line.strip().split(" +++$+++ ")
            if len(parts) == 4:
                conv_ids = eval(parts[3])
                for i in range(len(conv_ids) - 1):
                    q = lines.get(conv_ids[i], "")
                    a = lines.get(conv_ids[i+1], "")
                    if q and a:
                        pairs.append((q, a))
    return pairs

lines = load_lines()
pairs = load_conversations(lines)
print(f"Loaded {len(pairs)} dialog pairs.")

#basic tokenizer: lowercase, remove non-alphabetic characters
def tokenize(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9?.!,]+", " ", text)
    return text.strip().split()

#vocab from tokens
class Vocabulary:
    def __init__(self, min_freq=5):
        self.freq = Counter()
        self.word2idx = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.idx2word = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.min_freq = min_freq

    def build_vocab(self, pairs):
        for q, a in pairs:
            self.freq.update(tokenize(q))
            self.freq.update(tokenize(a))
        idx = 4
        for word, count in self.freq.items():
            if count >= self.min_freq:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                idx += 1

    def encode(self, text):
        return [self.word2idx.get(word, self.word2idx["<UNK>"]) for word in tokenize(text)]

    def decode(self, idxs):
        return ' '.join([self.idx2word.get(i, "<UNK>") for i in idxs])

vocab = Vocabulary(min_freq=5)
vocab.build_vocab(pairs)
print(f"Vocab size: {len(vocab.word2idx)}")

#Dataset for chat
class ChatDataset(Dataset):
    def __init__(self, pairs, vocab, max_len=20):
        self.data = []
        self.vocab = vocab
        self.max_len = max_len

        for q, a in pairs:
            enc_q = vocab.encode(q)[:max_len]
            enc_a = vocab.encode(a)[:max_len]
            if len(enc_q) > 1 and len(enc_a) > 1:
                self.data.append((enc_q, [vocab.word2idx["<SOS>"]] + enc_a + [vocab.word2idx["<EOS>"]]))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

#loader
def collate_fn(batch):
    questions, answers = zip(*batch)
    q_padded = pad_sequence([torch.tensor(x) for x in questions], batch_first=True, padding_value=0)
    a_padded = pad_sequence([torch.tensor(x) for x in answers], batch_first=True, padding_value=0)
    return q_padded.to(device), a_padded.to(device)

dataset = ChatDataset(pairs, vocab)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)

#bahdanau attention mechanism
class BahdanauAttention(nn.Module):
    def __init__(self, enc_hidden_size, dec_hidden_size):
        super().__init__()
        self.attn = nn.Linear(enc_hidden_size + dec_hidden_size, dec_hidden_size)
        self.v = nn.Parameter(torch.rand(dec_hidden_size))

    def forward(self, hidden, encoder_outputs, mask):
        batch_size = encoder_outputs.size(0)
        seq_len = encoder_outputs.size(1)

        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, hidden)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch, seq_len, hidden)
        energy = energy.transpose(1, 2)  # (batch, hidden, seq_len)
        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # (batch, 1, hidden)

        attention = torch.bmm(v, energy).squeeze(1)  # (batch, seq_len)
        attention = attention.masked_fill(mask == 0, -1e10)  # mask PADs
        return torch.softmax(attention, dim=1)  # (batch, seq_len)

#encoder
class Encoder(nn.Module):
    def __init__(self, input_size, embed_size, hidden_size, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)
        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        embedded = self.dropout(self.embedding(x))  # (batch, seq_len, embed)
        outputs, hidden = self.gru(embedded)  # outputs: (batch, seq_len, hidden), hidden: (1, batch, hidden)
        return outputs, hidden.squeeze(0)

#decoder with attention
class Decoder(nn.Module):
    def __init__(self, output_size, embed_size, hidden_size, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)
        self.attention = BahdanauAttention(hidden_size, hidden_size)
        self.gru = nn.GRU(embed_size + hidden_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_token, hidden, encoder_outputs, mask):
        input_token = input_token.unsqueeze(1)  # (batch, 1)
        embedded = self.dropout(self.embedding(input_token))  # (batch, 1, embed)

        attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, seq_len)
        attn_weights = attn_weights.unsqueeze(1)  # (batch, 1, seq_len)
        context = torch.bmm(attn_weights, encoder_outputs)  # (batch, 1, hidden)

        rnn_input = torch.cat((embedded, context), dim=2)  # (batch, 1, embed+hidden)
        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))  # output: (batch, 1, hidden)
        output = output.squeeze(1)
        context = context.squeeze(1)
        prediction = self.fc(torch.cat((output, context), dim=1))  # (batch, vocab_size)
        return prediction, hidden.squeeze(0), attn_weights.squeeze(1)

#masking tokens
def create_mask(seq):
    return (seq != 0).float()

#training
def train_model(encoder, decoder, dataloader, vocab, epochs=10, lr=0.001, teacher_forcing_ratio=0.5):
    enc_opt = optim.Adam(encoder.parameters(), lr=lr)
    dec_opt = optim.Adam(decoder.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    for epoch in range(epochs):
        encoder.train()
        decoder.train()
        total_loss = 0

        for batch_q, batch_a in dataloader:
            mask = create_mask(batch_q)

            enc_opt.zero_grad()
            dec_opt.zero_grad()

            encoder_outputs, hidden = encoder(batch_q)

            input_token = batch_a[:, 0]
            target = batch_a[:, 1:]
            batch_size, tgt_len = target.size()

            loss = 0
            for t in range(tgt_len):
                output, hidden, _ = decoder(input_token, hidden, encoder_outputs, mask)
                loss += criterion(output, target[:, t])
                use_teacher = random.random() < teacher_forcing_ratio
                top1 = output.argmax(1)
                input_token = target[:, t] if use_teacher else top1

            loss.backward()
            nn.utils.clip_grad_norm_(encoder.parameters(), 1)
            nn.utils.clip_grad_norm_(decoder.parameters(), 1)

            enc_opt.step()
            dec_opt.step()

            total_loss += loss.item() / tgt_len

        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}")

#save and load
def save_models(encoder, decoder, path="chatbot_model.pt"):
    torch.save({
        'encoder': encoder.state_dict(),
        'decoder': decoder.state_dict()
    }, path)

def load_models(path, encoder, decoder):
    checkpoint = torch.load(path)
    encoder.load_state_dict(checkpoint['encoder'])
    decoder.load_state_dict(checkpoint['decoder'])

#evaluation
def evaluate(encoder, decoder, vocab, sentence, max_len=20):
    encoder.eval()
    decoder.eval()

    with torch.no_grad():
        input_seq = torch.tensor([vocab.encode(sentence)], device=device)
        mask = create_mask(input_seq)
        enc_outs, hidden = encoder(input_seq)

        input_token = torch.tensor([vocab.word2idx["<SOS>"]], device=device)
        decoded = []

        for _ in range(max_len):
            output, hidden, _ = decoder(input_token, hidden, enc_outs, mask)
            top1 = output.argmax(1).item()
            if top1 == vocab.word2idx["<EOS>"]:
                break
            decoded.append(top1)
            input_token = torch.tensor([top1], device=device)

    return vocab.decode(decoded)

encoder = Encoder(len(vocab.word2idx), 300, 512).to(device)
decoder = Decoder(len(vocab.word2idx), 300, 512).to(device)
train_model(encoder, decoder, dataloader, vocab, epochs=10)
save_models(encoder, decoder)

print(evaluate(encoder, decoder, vocab, "hi there"))